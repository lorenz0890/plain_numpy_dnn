{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "from operator import add\n",
    "\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.datasets import mnist\n",
    "import numpy as np\n",
    "\n",
    "import pickle\n",
    "import timeit\n",
    "\n",
    "from tensorflow import keras # Required for Tensorboard\n",
    "from datetime import datetime # Required for Tensorboard\n",
    "from copy import deepcopy\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LeNet():\n",
    "    Convolution2D = tf.keras.layers.Convolution2D\n",
    "    MaxPooling2D = tf.keras.layers.MaxPooling2D\n",
    "    Flatten = tf.keras.layers.Flatten\n",
    "    Dense = tf.keras.layers.Dense\n",
    "\n",
    "    model = tf.keras.models.Sequential()\n",
    "\n",
    "    with tf.name_scope(\"LeNet\"):\n",
    "        with tf.name_scope(\"Convolution_Block\"):\n",
    "            # Add the first convolution layer\n",
    "            model.add(Convolution2D(\n",
    "                filters = 20,\n",
    "                kernel_size = (5, 5),\n",
    "                padding = \"same\",\n",
    "                input_shape = (28, 28, 1),\n",
    "                activation=\"relu\",\n",
    "                name=\"Conv1\"))\n",
    "\n",
    "            # Add a pooling layer\n",
    "            model.add(MaxPooling2D(\n",
    "                pool_size = (2, 2),\n",
    "                strides =  (2, 2),\n",
    "                name=\"MaxPool1\"))\n",
    "\n",
    "            # Add the second convolution layer\n",
    "            model.add(Convolution2D(\n",
    "                filters = 50,\n",
    "                kernel_size = (5, 5),\n",
    "                padding = \"same\",\n",
    "                activation=\"relu\",\n",
    "                name=\"Conv2\"))\n",
    "\n",
    "            # Add a second pooling layer\n",
    "            model.add(MaxPooling2D(\n",
    "                pool_size = (2, 2),\n",
    "                strides = (2, 2),\n",
    "                name=\"MaxPool2\"))\n",
    "\n",
    "        # Flatten the network\n",
    "        model.add(Flatten())\n",
    "\n",
    "        with tf.name_scope(\"Dense_Block\"):\n",
    "            # Add a fully-connected hidden layer\n",
    "            model.add(Dense(500,\n",
    "                activation=\"relu\",\n",
    "                name=\"Dense3\"))\n",
    "\n",
    "            # Add a fully-connected output layer\n",
    "            model.add(Dense(10,\n",
    "                activation=\"softmax\",\n",
    "                name=\"Dense4\"))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if MOD_WEIGHTS == True:\n",
    "    k = len(layers)\n",
    "    for i in range(0, k-1, 1):\n",
    "        fact = 1.0 + ((layers[i].W.shape[0]*layers[i].W.shape[1])/\n",
    "        (layers[i+1].W.shape[0]*layers[i+1].W.shape[1]))\n",
    "        layers[k-1-i].W*=fact\n",
    "        print(fact)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "315/315 [==============================] - 115s 364ms/step - loss: 2.2791 - accuracy: 0.2540 - val_loss: 2.2588 - val_accuracy: 0.3472\n"
     ]
    }
   ],
   "source": [
    "model = LeNet()\n",
    "\n",
    "# Instantiate an optimizer to train the model.\n",
    "optimizer = keras.optimizers.SGD(learning_rate=1e-3)\n",
    "\n",
    "# Compile the network\n",
    "model.compile(\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    optimizer=optimizer,\n",
    "    metrics=[\"accuracy\"],\n",
    "    run_eagerly=False\n",
    ")\n",
    "\n",
    "AUTOTRAIN = True\n",
    "if AUTOTRAIN:\n",
    "    \n",
    "    dataset = mnist.load_data()\n",
    "\n",
    "    train_data = dataset[0][0]\n",
    "    train_labels = dataset[0][1]\n",
    "    test_data = dataset[1][0]\n",
    "    test_labels = dataset[1][1]\n",
    "\n",
    "    # Reshape the data to a (70000, 28, 28, 1) tensord\n",
    "    train_data = train_data.reshape([*train_data.shape,1]) / 255.0\n",
    "    test_data = test_data.reshape([*test_data.shape,1]) / 255.0\n",
    "    train_labels = np.eye(10)[train_labels]\n",
    "\n",
    "    # Tranform test labels to one-hot encoding\n",
    "    test_labels = np.eye(10)[test_labels]\n",
    "\n",
    "    hist = model.fit(\n",
    "            train_data,\n",
    "            train_labels,\n",
    "            batch_size=128,\n",
    "            epochs=1,\n",
    "            validation_split=0.33,\n",
    "            verbose=1,\n",
    "            callbacks=[])\n",
    "else:\n",
    "    # TODO:\n",
    "    # https://keras.io/guides/writing_a_training_loop_from_scratch/\n",
    "    # https://keras.io/api/losses/probabilistic_losses/#categoricalcrossentropy-class\n",
    "    batch_size = 128\n",
    "    (x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "    x_train = x_train.reshape([*x_train.shape,1]) / 255.0 ##np.reshape(x_train, (-1, 784)) \n",
    "    x_test = x_test.reshape([*x_test.shape,1]) / 255.0\n",
    "    \n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "    train_dataset = train_dataset.shuffle(buffer_size=1024).batch(batch_size)\n",
    "    \n",
    "    x_val = x_train[-1000:]\n",
    "    y_val = y_train[-1000:]\n",
    "    x_train = x_train[:-100000]\n",
    "    y_train = y_train[:-100000]\n",
    "    val_dataset = tf.data.Dataset.from_tensor_slices((x_val, y_val))\n",
    "    val_dataset = val_dataset.batch(128)\n",
    "    \n",
    "    # Instantiate a loss function.\n",
    "    loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "    \n",
    "    # Prepare the metrics.\n",
    "    train_acc_metric = keras.metrics.SparseCategoricalAccuracy()\n",
    "    val_acc_metric = keras.metrics.SparseCategoricalAccuracy()\n",
    "    \n",
    "    @tf.function\n",
    "    def train_step(x,y):\n",
    "        # Open a GradientTape to record the operations run\n",
    "        # during the forward pass, which enables autodifferentiation.\n",
    "        with tf.GradientTape() as tape:\n",
    "\n",
    "            # Run the forward pass of the layer.\n",
    "            # The operations that the layer applies\n",
    "            # to its inputs are going to be recorded\n",
    "            # on the GradientTape.\n",
    "            logits = model(x_batch_train, training=True)  # Logits for this minibatch\n",
    "\n",
    "            # Compute the loss value for this minibatch.\n",
    "            loss_value = loss_fn(y_batch_train, logits)\n",
    "\n",
    "        # Use the gradient tape to automatically retrieve\n",
    "        # the gradients of the trainable variables with respect to the loss.\n",
    "        grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "        train_acc_metric.update_state(y_batch_train, logits)\n",
    "        return loss_value\n",
    "    \n",
    "    @tf.function\n",
    "    def test_step(x, y):\n",
    "        val_logits = model(x, training=False)\n",
    "        # Update val metrics\n",
    "        val_acc_metric.update_state(y, val_logits)\n",
    "\n",
    "\n",
    "    epochs = 3\n",
    "    for epoch in range(epochs):\n",
    "        print(\"\\nStart of epoch %d\" % (epoch,))\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Iterate over the batches of the dataset.\n",
    "        for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
    "\n",
    "            \n",
    "            loss_value = train_step(x_batch_train, y_batch_train)\n",
    "            \n",
    "            # Log every 200 batches.\n",
    "            if step % 200 == 0:\n",
    "                print(\n",
    "                    \"Training loss (for one batch) at step %d: %.4f\"\n",
    "                    % (step, float(loss_value))\n",
    "                )\n",
    "                print(\"Seen so far: %s samples\" % ((step + 1) * batch_size))\n",
    "\n",
    "        # Display metrics at the end of each epoch.\n",
    "        train_acc = train_acc_metric.result()\n",
    "        print(\"Training acc over epoch: %.4f\" % (float(train_acc),))\n",
    "\n",
    "        # Reset training metrics at the end of each epoch\n",
    "        train_acc_metric.reset_states()\n",
    "\n",
    "        # Run a validation loop at the end of each epoch.\n",
    "        for x_batch_val, y_batch_val in val_dataset:\n",
    "            test_step(x_batch_val, y_batch_val)\n",
    "        val_acc = val_acc_metric.result()\n",
    "        val_acc_metric.reset_states()\n",
    "        print(\"Validation acc: %.4f\" % (float(val_acc),))\n",
    "        print(\"Time taken: %.2fs\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79/79 [==============================] - 6s 79ms/step - loss: 2.3019 - accuracy: 0.1060\n"
     ]
    }
   ],
   "source": [
    "test_labels = np.eye(10)[y_test]\n",
    "(loss, accuracy) = model.evaluate(\n",
    "            x_test,\n",
    "            test_labels,\n",
    "            batch_size=128,\n",
    "            verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lenet.layers\n",
    "inv_layers = model.layers#.reverse()\n",
    "inv_layers.reverse()\n",
    "for i, layer in enumerate(model.layers):\n",
    "    print(layer.name)\n",
    "    try:\n",
    "        cce = tf.keras.losses.CategoricalCrossentropy()\n",
    "        model.layers[i].set_weights([layer.get_weights()[0]*1.0, layer.get_weights()[1]])\n",
    "    except Exception as e:\n",
    "        print('Failed for', layer.name)\n",
    "        print(e)\n",
    "        \n",
    "print(model.layers[5].get_weights()[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tquant)",
   "language": "python",
   "name": "tquant"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
